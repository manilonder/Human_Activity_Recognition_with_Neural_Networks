{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqfkvEmgg5NU"
      },
      "outputs": [],
      "source": [
        "\n",
        "######################################################################################\n",
        "######################################################################################\n",
        "################################      Question 3     #####################################\n",
        "######################################################################################\n",
        "######################################################################################\n",
        "import h5py\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sn\n",
        "import time\n",
        "import sys\n",
        "\n",
        "\n",
        "class humanActivityRecognition(object):\n",
        "\n",
        "\n",
        "    def __init__(self, size, part_Q):\n",
        "\n",
        "        self.part_Q = part_Q\n",
        "        self.size = size\n",
        "        self.sizeOfLayer = len(size) - 1\n",
        "        self.mlp_sizeOfLayer = None\n",
        "        self.mlp_coeff, self.coeffOf_firstLayer, self.mlp_action, self.actionOf_firstLayer = None, None, None, None\n",
        "        self.init_coeff()\n",
        "\n",
        "\n",
        "    def init_coeff(self):\n",
        "        \"\"\"\n",
        "        This is a function that appears to be initializing the coefficients \n",
        "        (also known as parameters) of a machine learning model. It looks like \n",
        "        the function is part of a class, as it uses the self keyword to access \n",
        "        class attributes.\n",
        "\n",
        "        The function appears to be implementing initialization for a multi-layer\n",
        "        perceptron (MLP) and also for the first layer of the model. It looks like \n",
        "        the initialization for the first layer depends on the value of the part_Q \n",
        "        attribute of the class, which can be 1, 2, or 3.\n",
        "\n",
        "        The function uses the np.random.uniform function to initialize the \n",
        "        coefficients using the Xavier distribution, which is a common initialization \n",
        "        technique that helps prevent the vanishing and exploding gradient problems \n",
        "        in deep neural networks. The function also initializes momentum, which is\n",
        "        a hyperparameter used in optimization algorithms such as stochastic \n",
        "        gradient descent (SGD) with momentum.\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        part_Q = self.part_Q\n",
        "        size = self.size\n",
        "        sizeOfLayer = self.sizeOfLayer\n",
        "\n",
        "        W = []\n",
        "        b = []\n",
        "        for i in range(1, sizeOfLayer):\n",
        "            \n",
        "            r = np.sqrt(6 / (size[i] + size[i + 1]))\n",
        "            W.append(np.random.uniform(-r, r, size=(size[i], size[i + 1])))\n",
        "            b.append(np.zeros((1, size[i + 1])))\n",
        "\n",
        "        self.mlp_sizeOfLayer = len(W)\n",
        "        coeff = {\"W\": W, \"b\": b}\n",
        "        action = {\"W\": [0] * self.mlp_sizeOfLayer, \"b\": [0] * self.mlp_sizeOfLayer}\n",
        "        self.mlp_coeff = coeff\n",
        "        self.mlp_action = action\n",
        "\n",
        "        N = size[0]\n",
        "        H = size[1]\n",
        "        Z = N + H\n",
        "\n",
        "        if part_Q == 1:\n",
        "            r = np.sqrt(6 / (N + H))\n",
        "            Wih = np.random.uniform(-r, r, size=(N, H))\n",
        "            r = np.sqrt(6 / (H + H))\n",
        "            Whh = np.random.uniform(-r, r, size=(H, H))\n",
        "            b = np.zeros((1, H))\n",
        "\n",
        "            coeff = {\"Wih\": Wih, \"Whh\": Whh, \"b\": b}\n",
        "\n",
        "        if part_Q == 2:\n",
        "            r = np.sqrt(6 / (Z + H))\n",
        "\n",
        "            Wf = np.random.uniform(-r, r, size=(Z, H))\n",
        "            Wi = np.random.uniform(-r, r, size=(Z, H))\n",
        "            Wc = np.random.uniform(-r, r, size=(Z, H))\n",
        "            Wo = np.random.uniform(-r, r, size=(Z, H))\n",
        "\n",
        "            bf = np.zeros((1, H))\n",
        "            bi = np.zeros((1, H))\n",
        "            bc = np.zeros((1, H))\n",
        "            bo = np.zeros((1, H))\n",
        "\n",
        "            coeff = {\"Wf\": Wf, \"bf\": bf,\n",
        "                      \"Wi\": Wi, \"bi\": bi,\n",
        "                      \"Wc\": Wc, \"bc\": bc,\n",
        "                      \"Wo\": Wo, \"bo\": bo}\n",
        "\n",
        "        if part_Q == 3:\n",
        "            rN = np.sqrt(6 / (N + H))\n",
        "            rH = np.sqrt(6 / (H + H))\n",
        "\n",
        "            Wz = np.random.uniform(-rN, rN, size=(N, H))\n",
        "            Uz = np.random.uniform(-rH, rH, size=(H, H))\n",
        "            bz = np.zeros((1, H))\n",
        "\n",
        "            Wr = np.random.uniform(-rN, rN, size=(N, H))\n",
        "            Ur = np.random.uniform(-rH, rH, size=(H, H))\n",
        "            br = np.zeros((1, H))\n",
        "\n",
        "            Wh = np.random.uniform(-rN, rN, size=(N, H))\n",
        "            Uh = np.random.uniform(-rH, rH, size=(H, H))\n",
        "            bh = np.zeros((1, H))\n",
        "\n",
        "            coeff = {\"Wz\": Wz, \"Uz\": Uz, \"bz\": bz,\n",
        "                      \"Wr\": Wr, \"Ur\": Ur, \"br\": br,\n",
        "                      \"Wh\": Wh, \"Uh\": Uh, \"bh\": bh}\n",
        "\n",
        "\n",
        "        action = dict.fromkeys(coeff.keys(), 0)\n",
        "        self.coeffOf_firstLayer = coeff\n",
        "        self.actionOf_firstLayer = action\n",
        "\n",
        "\n",
        "    def train(self, X, Y, eta, alpha, batch_size, epoch):\n",
        "        \"\"\"\n",
        "        This is a method for training a neural network using mini-batch gradient\n",
        "         descent. The method takes the following arguments:\n",
        "\n",
        "        X: a 2D array of input data, where each row represents a sample and each \n",
        "        column represents a feature.\n",
        "        Y: a 2D array of labels for the input data, where each row represents a \n",
        "        sample and each column represents a label.\n",
        "        eta: the learning rate.\n",
        "        alpha: the momentum coefficient.\n",
        "        batch_size: the size of the mini-batches used to train the model.\n",
        "        epoch: the number of epochs to train the model.\n",
        "        The method first divides the data into a training set and a validation \n",
        "        set, and then trains the model on the training set using mini-batches. \n",
        "        It tracks the training loss, validation loss, training accuracy, and \n",
        "        validation accuracy over the course of training and returns these values \n",
        "        in a dictionary at the end of training. The method also has an early \n",
        "        stopping mechanism that stops training if the validation loss reaches \n",
        "        convergence, defined as being within a certain range of the average \n",
        "        validation loss over the past 15 epochs.\n",
        "        \"\"\"\n",
        "\n",
        "        listof_trainLoss = []\n",
        "        listOf_valueLoss = []\n",
        "        listOf_trainAccuracy = []\n",
        "        listOf_valueAccuracy = []\n",
        "\n",
        "        # create validation set\n",
        "        sizeOfValue = int(X.shape[0] / 10)\n",
        "        p = np.random.permutation(X.shape[0])\n",
        "        valueOfX = X[p][:sizeOfValue]\n",
        "        valueOfY = Y[p][:sizeOfValue]\n",
        "        X = X[p][sizeOfValue:]\n",
        "        Y = Y[p][sizeOfValue:]\n",
        "\n",
        "        sample_size = X.shape[0]\n",
        "        iterationOf_perEpoch = int(sample_size / batch_size)\n",
        "\n",
        "        for i in range(epoch):\n",
        "\n",
        "            time_start = time.time()\n",
        "\n",
        "            start = 0\n",
        "            end = batch_size\n",
        "            p = np.random.permutation(X.shape[0])\n",
        "            X = X[p]\n",
        "            Y = Y[p]\n",
        "\n",
        "            for j in range(iterationOf_perEpoch):\n",
        "\n",
        "                X_batch = X[start:end]\n",
        "                Y_batch = Y[start:end]\n",
        "\n",
        "              \n",
        "                pred, o, drv, h, h_derivative, memory = self.forward_propagation_pass(X_batch)\n",
        "\n",
        "              \n",
        "                delta = pred\n",
        "                delta[Y_batch == 1] -= 1\n",
        "                delta = delta / batch_size\n",
        "\n",
        "                \n",
        "                fl_grads, mlp_grads = self.Backpropagation_pass(X_batch, o, drv, delta, h, h_derivative, memory)\n",
        "\n",
        "                \n",
        "                self.update_coeff(eta, alpha, fl_grads, mlp_grads)\n",
        "\n",
        "                start = end\n",
        "                end += batch_size\n",
        "\n",
        "           \n",
        "            pred = self.predict(X, acc=False)\n",
        "            lossOfTrain = self.cross_entropy(Y, pred)\n",
        "\n",
        "      \n",
        "            accuracyOfTrain = self.predict(X, Y, acc=True)\n",
        "\n",
        "           \n",
        "            accuracyOfValue = self.predict(valueOfX, valueOfY, acc=True)\n",
        "\n",
        "            # val loss\n",
        "            pred = self.predict(valueOfX, acc=False)\n",
        "            valueOfLoss = self.cross_entropy(valueOfY, pred)\n",
        "\n",
        "\n",
        "            print('Train Loss: %.2f, Val Loss: %.2f, Train Acc: %.2f, Val Acc: %.2f [Epoch: %d of %d]'\n",
        "                  % (lossOfTrain, valueOfLoss, accuracyOfTrain, accuracyOfValue, i + 1, epoch))\n",
        "\n",
        "            listof_trainLoss.append(lossOfTrain)\n",
        "            listOf_valueLoss.append(valueOfLoss)\n",
        "            listOf_trainAccuracy.append(accuracyOfTrain)\n",
        "            listOf_valueAccuracy.append(accuracyOfValue)\n",
        "\n",
        "\n",
        "            # stop if the cross entropy of validation set converged\n",
        "            if i > 15:\n",
        "                conv = listOf_valueLoss[-16:-1]\n",
        "                conv = sum(conv) / len(conv)\n",
        "\n",
        "                limit = 0.02\n",
        "                if (conv - limit) < valueOfLoss < (conv + limit):\n",
        "                    print(\"\\nTraining stopped since validation C-E reached convergence.\")\n",
        "                    return {\"listof_trainLoss\": listof_trainLoss, \"listOf_valueLoss\": listOf_valueLoss,\n",
        "                            \"listOf_trainAccuracy\": listOf_trainAccuracy, \"listOf_valueAccuracy\": listOf_valueAccuracy}\n",
        "\n",
        "\n",
        "        return {\"listof_trainLoss\": listof_trainLoss, \"listOf_valueLoss\": listOf_valueLoss,\n",
        "                \"listOf_trainAccuracy\": listOf_trainAccuracy, \"listOf_valueAccuracy\": listOf_valueAccuracy}\n",
        "\n",
        "\n",
        "    def forward_propagation_pass(self, X):\n",
        "        \"\"\"\n",
        "        This is a method for performing a forward pass through a neural network. \n",
        "        The method takes as input a 2D array X of input data and returns the \n",
        "        following outputs:\n",
        "\n",
        "        pred: the predicted output of the model.\n",
        "        o: a list of the functionOfActivations of the hidden layers.\n",
        "        drv: a list of the derivatives of the functionOfActivations of the hidden layers.\n",
        "        h: the hidden state of the recurrent layer.\n",
        "        h_derivative: the resultOfDerivation of the hidden state of the recurrent layer.\n",
        "        memory: the memory of the recurrent layer.\n",
        "        The method first checks the type of the recurrent layer (either \n",
        "        RNN, LSTM, or GRU) and computes the hidden state h and its resultOfDerivation \n",
        "        h_derivative using the appropriate method. It then applies the ReLU functionOfActivation \n",
        "        function to the hidden state and computes the output of the model using \n",
        "        the softmax functionOfActivation function. The functionOfActivations and derivatives of the \n",
        "        hidden layers are stored in the o and drv lists, respectively. The memory \n",
        "        of the recurrent layer is also stored.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        part_Q = self.part_Q\n",
        "        P_ofMlp = self.mlp_coeff\n",
        "        p_fl = self.coeffOf_firstLayer\n",
        "\n",
        "        o = []\n",
        "        drv = []\n",
        "\n",
        "        h = 0\n",
        "        h_derivative = 0\n",
        "        memory = 0\n",
        "\n",
        "        # first layer\n",
        "        if part_Q == 1:\n",
        "            h, h_derivative = self.forward_recurrent(X, p_fl)\n",
        "            o.append(h[:, -1, :])\n",
        "            drv.append(h_derivative[:, -1, :])\n",
        "        if part_Q == 2:\n",
        "            h, memory = self.forward_lstm(X, p_fl)\n",
        "            o.append(h)\n",
        "            drv.append(1)\n",
        "        if part_Q == 3:\n",
        "            h, memory = self.forward_gru(X, p_fl)\n",
        "            o.append(h)\n",
        "            drv.append(1)\n",
        "\n",
        "        # relu layers\n",
        "        for i in range(self.mlp_sizeOfLayer - 1):\n",
        "            functionOfActivation, resultOfDerivation = self.forward_perceptron(o[-1], P_ofMlp[\"W\"][i], P_ofMlp[\"b\"][i], \"relu\")\n",
        "            o.append(functionOfActivation)\n",
        "            drv.append(resultOfDerivation)\n",
        "\n",
        "        # output layer\n",
        "        pred = self.forward_perceptron(o[-1], P_ofMlp[\"W\"][-1], P_ofMlp[\"b\"][-1], \"softmax\")[0]\n",
        "\n",
        "        return pred, o, drv, h, h_derivative, memory\n",
        "\n",
        "\n",
        "    def Backpropagation_pass(self, X, o, drv, delta, h=None, h_derivative=None, memory=None):\n",
        "        \"\"\"\n",
        "        This is a method for performing a backward pass through a neural network \n",
        "        to compute the gradients of the model's parameters. The method takes the \n",
        "        following arguments:\n",
        "\n",
        "        X: a 2D array of input data, where each row represents a sample and each\n",
        "         column represents a feature.\n",
        "        o: a list of the functionOfActivations of the hidden layers.\n",
        "        drv: a list of the derivatives of the functionOfActivations of the hidden layers.\n",
        "        delta: the error of the output layer.\n",
        "        h: the hidden state of the recurrent layer (optional, only used for RNN).\n",
        "        h_derivative: the resultOfDerivation of the hidden state of the recurrent layer (optional,\n",
        "        only used for  RNN).\n",
        "        memory: the memory of the recurrent layer (optional, only used for LSTM and GRU).\n",
        "        The method first uses the error of the output layer to compute the gradients of \n",
        "        the parameters of the output layer and the fully-connected layers. It then \n",
        "        backpropagates the error through the fully-connected layers using the functionOfActivations \n",
        "        and derivatives stored in the o and drv lists. Finally, it computes the gradients \n",
        "        of the parameters of the recurrent layer using the appropriate method depending \n",
        "        on the type of the recurrent layer (either RNN, LSTM, or GRU). The method \n",
        "        returns the gradients of the parameters of the recurrent layer and the fully-connected layers.\n",
        "                \"\"\"\n",
        "        part_Q = self.part_Q\n",
        "        p_fl = self.coeffOf_firstLayer\n",
        "        P_ofMlp = self.mlp_coeff\n",
        "\n",
        "        fl_grads = dict.fromkeys(p_fl.keys())\n",
        "        mlp_grads = {\"W\": [0] * self.mlp_sizeOfLayer, \"b\": [0] * self.mlp_sizeOfLayer}\n",
        "\n",
        "        # backpropagation until recurrent\n",
        "        for i in reversed(range(self.mlp_sizeOfLayer)):\n",
        "            mlp_grads[\"W\"][i], mlp_grads[\"b\"][i], delta = self.backward_perceptron(P_ofMlp[\"W\"][i], o[i], drv[i], delta)\n",
        "\n",
        "        # backpropagation through time\n",
        "        if part_Q == 1:\n",
        "            fl_grads = self.backward_recurrent(X, h, h_derivative, delta, p_fl)\n",
        "        if part_Q == 2:\n",
        "            fl_grads = self.backward_lstm(memory, p_fl, delta)\n",
        "        if part_Q == 3:\n",
        "            fl_grads = self.backward_gru(X, memory, p_fl, delta)\n",
        "\n",
        "        return fl_grads, mlp_grads\n",
        "\n",
        "\n",
        "    def update_coeff(self, eta, alpha, fl_grads, mlp_grads):\n",
        "       \n",
        "        \"\"\"\n",
        "        This is a method for updating the model's parameters using the gradients\n",
        "         computed in the backward pass. The method takes the following arguments:\n",
        "\n",
        "        eta: the learning rate.\n",
        "        alpha: the momentum coefficient.\n",
        "        fl_grads: a dictionary of the gradients of the parameters of the recurrent layer.\n",
        "        mlp_grads: a dictionary of the gradients of the parameters of the fully-connected layers.\n",
        "        \"\"\"\n",
        "\n",
        "       \n",
        "        p_fl = self.coeffOf_firstLayer\n",
        "        fl_m = self.actionOf_firstLayer\n",
        "        P_ofMlp = self.mlp_coeff\n",
        "        mlp_m = self.mlp_action\n",
        "\n",
        " \n",
        "        for p in self.coeffOf_firstLayer:\n",
        "            fl_m[p] = eta * fl_grads[p] + alpha * fl_m[p]\n",
        "            p_fl[p] -= fl_m[p]\n",
        "\n",
        "        \n",
        "        for i in range(self.mlp_sizeOfLayer):\n",
        "            mlp_m[\"W\"][i] = eta * mlp_grads[\"W\"][i] + alpha * mlp_m[\"W\"][i]\n",
        "            mlp_m[\"b\"][i] = eta * mlp_grads[\"b\"][i] + alpha * mlp_m[\"b\"][i]\n",
        "            P_ofMlp[\"W\"][i] -= mlp_m[\"W\"][i]\n",
        "            P_ofMlp[\"b\"][i] -= mlp_m[\"b\"][i]\n",
        "\n",
        "       \n",
        "        self.coeffOf_firstLayer = p_fl\n",
        "        self.actionOf_firstLayer = fl_m\n",
        "        self.mlp_coeff = P_ofMlp\n",
        "        self.mlp_action = mlp_m\n",
        "\n",
        "\n",
        "    def forward_perceptron(self, X, W, b, a):\n",
        "       \n",
        "        u = X @ W + b\n",
        "        return self.functionOfActivation(u, a)\n",
        "\n",
        "\n",
        "    def backward_perceptron(self, W, o, drv, delta):\n",
        "       \n",
        "        dW = o.T @ delta\n",
        "        db = delta.sum(axis=0, keepdims=True)\n",
        "        delta = drv * (delta @ W.T)\n",
        "        return dW, db, delta\n",
        "\n",
        "\n",
        "    def forward_recurrent(self, X, p_fl):\n",
        "        \"\"\"\n",
        "        This is a method for performing a forward pass through a RNN \n",
        "        layer. The method takes as input a 3D array X of input data and a dictionary \n",
        "        p_fl of the parameters of the recurrent layer, and returns the following \n",
        "        outputs:\n",
        "\n",
        "        h: the hidden state of the recurrent layer.\n",
        "        h_drv: the resultOfDerivation of the hidden state of the recurrent layer.\n",
        "        The method loops over the time steps of the input data and computes the \n",
        "        hidden state h and its resultOfDerivation h_drv using the tanh functionOfActivation function. \n",
        "        The hidden state at each time step is computed using the previous hidden\n",
        "        state and the current input, as well as the weights Wih and Whh and the \n",
        "        bias term b of the recurrent layer. The resultOfDerivation of the hidden state \n",
        "        is computed using the resultOfDerivation of the tanh function. The method returns\n",
        "        the hidden state and its resultOfDerivation for each time step.\n",
        "        \"\"\"\n",
        "\n",
        "        N, T, D = X.shape\n",
        "        H = self.size[1]\n",
        "\n",
        "        Wih = p_fl[\"Wih\"]\n",
        "        Whh = p_fl[\"Whh\"]\n",
        "        b = p_fl[\"b\"]\n",
        "\n",
        "        previousOf_h = np.zeros((N, H))\n",
        "        h = np.empty((N, T, H))\n",
        "        h_derivative = np.empty((N, T, H))\n",
        "\n",
        "        for t in range(T):\n",
        "            x = X[:, t, :]\n",
        "            u = x @ Wih + previousOf_h @ Whh + b\n",
        "            h[:, t, :], h_derivative[:, t, :] = self.functionOfActivation(u, \"tanh\")\n",
        "            previousOf_h = h[:, t, :]\n",
        "\n",
        "        return h, h_derivative\n",
        "\n",
        "\n",
        "    def backward_recurrent(self, X, h, h_derivative, delta, p_fl):\n",
        "        \"\"\"\n",
        "        This is a method for performing a backward pass through a RNN layer to \n",
        "        compute the gradients of the layer's parameters. The method takes the \n",
        "        following arguments:\n",
        "\n",
        "        X: a 3D array of input data, where the first dimension represents the batch\n",
        "        size, the second dimension represents the time steps, and the third dimension\n",
        "        represents the features.\n",
        "        h: the hidden state of the recurrent layer.\n",
        "        h_derivative: the resultOfDerivation of the hidden state of the recurrent layer.\n",
        "        delta: the error of the output layer.\n",
        "        p_fl: a dictionary of the parameters of the recurrent layer.\n",
        "        The method loops over the time steps in reverse order and computes the \n",
        "        gradients of the parameters Wih, Whh, and b using the error delta and the\n",
        "        hidden state and its resultOfDerivation at each time step. The error at each time\n",
        "        step is computed by backpropagating the error from the next time step\n",
        "        using the weights Whh of the recurrent layer. The method returns the \n",
        "        gradients of the parameters as a dictionary.\n",
        "        \"\"\"\n",
        "\n",
        "        N, T, D = X.shape\n",
        "        H = self.size[1]\n",
        "\n",
        "        Whh = p_fl[\"Whh\"]\n",
        "\n",
        "        dWih = 0\n",
        "        dWhh = 0\n",
        "        db = 0\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            x = X[:, t, :]\n",
        "\n",
        "            if t > 0:\n",
        "                previousOf_h = h[:, t - 1, :]\n",
        "                previousOf_h_derv = h_derivative[:, t - 1, :]\n",
        "            else:\n",
        "                previousOf_h = np.zeros((N, H))\n",
        "                previousOf_h_derv = 0\n",
        "\n",
        "            dWih += x.T @ delta\n",
        "            dWhh += previousOf_h.T @ delta\n",
        "            db += delta.sum(axis=0, keepdims=True)\n",
        "            delta = previousOf_h_derv * (delta @ Whh)\n",
        "\n",
        "        return {\"Wih\": dWih, \"Whh\": dWhh, \"b\": db}\n",
        "\n",
        "\n",
        "    def forward_lstm(self, X, p_fl):\n",
        "        \"\"\"\n",
        "        The LSTM is a type of recurrent neural network that is often used for \n",
        "        modeling sequential data.\n",
        "\n",
        "        In this implementation, the input X is a three-dimensional tensor with dimensions\n",
        "        N, T, and D, where N is the batch size, T is the sequence length, and D is the \n",
        "        input feature dimension. The p_fl argument is a dictionary that contains the weight \n",
        "        matrices and biases for the LSTM. The function returns the hidden state of the LSTM \n",
        "        at the final time step and a memory containing intermediate values that are needed \n",
        "        for backpropagation.\n",
        "\n",
        "        The LSTM computes the hidden state at each time step t by using the hidden state at\n",
        "        the previous time step, the input at the current time step, and some other \n",
        "        intermediate values. These intermediate values are computed using element-wise\n",
        "        multiplication and non-linear functionOfActivation functions such as sigmoid and tanh. \n",
        "        The final hidden state at time T is returned as the output of the function.\n",
        "        \"\"\"\n",
        "\n",
        "        N, T, D = X.shape\n",
        "        H = self.size[1]\n",
        "\n",
        "        Wf, bf = p_fl[\"Wf\"], p_fl[\"bf\"]\n",
        "        Wi, bi = p_fl[\"Wi\"], p_fl[\"bi\"]\n",
        "        Wc, bc = p_fl[\"Wc\"], p_fl[\"bc\"]\n",
        "        Wo, bo = p_fl[\"Wo\"], p_fl[\"bo\"]\n",
        "\n",
        "        previousOf_h = np.zeros((N, H))\n",
        "        previousOf_c = np.zeros((N, H))\n",
        "        z = np.empty((N, T, D + H))\n",
        "        c = np.empty((N, T, H))\n",
        "        tanhc = np.empty((N, T, H))\n",
        "        hf = 0\n",
        "        hi = np.empty((N, T, H))\n",
        "        hc = np.empty((N, T, H))\n",
        "        ho = np.empty((N, T, H))\n",
        "        tanhc_d = np.empty((N, T, H))\n",
        "        hf_d = np.empty((N, T, H))\n",
        "        hi_d = np.empty((N, T, H))\n",
        "        hc_d = np.empty((N, T, H))\n",
        "        ho_d = np.empty((N, T, H))\n",
        "\n",
        "        for t in range(T):\n",
        "            z[:, t, :] = np.column_stack((previousOf_h, X[:, t, :]))\n",
        "            z_cur = z[:, t, :]\n",
        "\n",
        "            hf, hf_d[:, t, :] = self.functionOfActivation(z_cur @ Wf + bf, \"sigmoid\")\n",
        "            hi[:, t, :], hi_d[:, t, :] = self.functionOfActivation(z_cur @ Wi + bi, \"sigmoid\")\n",
        "            hc[:, t, :], hc_d[:, t, :] = self.functionOfActivation(z_cur @ Wc + bc, \"tanh\")\n",
        "            ho[:, t, :], ho_d[:, t, :] = self.functionOfActivation(z_cur @ Wo + bo, \"sigmoid\")\n",
        "\n",
        "            c[:, t, :] = hf * previousOf_c + hi[:, t, :] * hc[:, t, :]\n",
        "            tanhc[:, t, :], tanhc_d[:, t, :] = self.functionOfActivation(c[:, t, :], \"tanh\")\n",
        "            previousOf_h = ho[:, t, :] * tanhc[:, t, :]\n",
        "            previousOf_c = c[:, t, :]\n",
        "\n",
        "            memory = {\"z\": z,\n",
        "                     \"c\": c,\n",
        "                     \"tanhc\": (tanhc, tanhc_d),\n",
        "                     \"hf_d\": hf_d,\n",
        "                     \"hi\": (hi, hi_d),\n",
        "                     \"hc\": (hc, hc_d),\n",
        "                     \"ho\": (ho, ho_d)}\n",
        "\n",
        "        return previousOf_h, memory\n",
        "\n",
        "\n",
        "    def backward_lstm(self, memory, p_fl, delta):\n",
        "        \"\"\"\n",
        "      This is a function for implementing the backward pass of an LSTM (Long \n",
        "      Short-Term Memory) network, a type of recurrent neural network. In the backward pass,\n",
        "      the function is responsible for computing the gradients of the loss function with \n",
        "      respect to the model's parameters (i.e., the weights and biases of the LSTM).\n",
        "\n",
        "      The function takes in several arguments:\n",
        "\n",
        "      memory: a dictionary containing the functionOfActivations and intermediate values \n",
        "      computed during the forward pass of the LSTM.\n",
        "      p_fl: a dictionary containing the weights and biases of the LSTM.\n",
        "      delta: the error gradient with respect to the output of the LSTM at the \n",
        "      final time step.\n",
        "      The function first unpacks the weights and biases from the p_fl dictionary.\n",
        "      It then loops through the time steps in the reverse order, starting from the \n",
        "      final time step and going back to the first time step. At each time step, it \n",
        "      computes the gradients of the loss function with respect to the four \"gates\" \n",
        "      in the LSTM (i.e., the forget gate, input gate, output gate, and cell state). \n",
        "      It then updates the gradients of the weights and biases by adding the gradients \n",
        "      at that time step. Finally, it updates the error gradient delta by adding up the \n",
        "      gradients with respect to the input to the LSTM at that time step.\n",
        "\n",
        "        \"\"\"\n",
        "        # unpack variables\n",
        "        Wf = p_fl[\"Wf\"]\n",
        "        Wi = p_fl[\"Wi\"]\n",
        "        Wc = p_fl[\"Wc\"]\n",
        "        Wo = p_fl[\"Wo\"]\n",
        "\n",
        "        z = memory[\"z\"]\n",
        "        c = memory[\"c\"]\n",
        "        tanhc, tanhc_d = memory[\"tanhc\"]\n",
        "        hf_d = memory[\"hf_d\"]\n",
        "        hi, hi_d = memory[\"hi\"]\n",
        "        hc, hc_d = memory[\"hc\"]\n",
        "        ho, ho_d = memory[\"ho\"]\n",
        "\n",
        "        H = self.size[1]\n",
        "        T = z.shape[1]\n",
        "\n",
        "        # initialize gradients to zero\n",
        "        dWf = 0\n",
        "        dWi = 0\n",
        "        dWc = 0\n",
        "        dWo = 0\n",
        "        dbf = 0\n",
        "        dbi = 0\n",
        "        dbc = 0\n",
        "        dbo = 0\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "\n",
        "            z_cur = z[:, t, :]\n",
        "\n",
        "        \n",
        "            if t > 0:\n",
        "                previousOf_c = c[:, t - 1, :]\n",
        "            else:\n",
        "                previousOf_c = 0\n",
        "\n",
        "            dc = delta * ho[:, t, :] * tanhc_d[:, t, :]\n",
        "            dhf = dc * previousOf_c * hf_d[:, t, :]\n",
        "            dhi = dc * hc[:, t, :] * hi_d[:, t, :]\n",
        "            dhc = dc * hi[:, t, :] * hc_d[:, t, :]\n",
        "            dho = delta * tanhc[:, t, :] * ho_d[:, t, :]\n",
        "\n",
        "        \n",
        "            dWf += z_cur.T @ dhf\n",
        "            dbf += dhf.sum(axis=0, keepdims=True)\n",
        "\n",
        "            dWi += z_cur.T @ dhi\n",
        "            dbi += dhi.sum(axis=0, keepdims=True)\n",
        "\n",
        "            dWc += z_cur.T @ dhc\n",
        "            dbc += dhc.sum(axis=0, keepdims=True)\n",
        "\n",
        "            dWo += z_cur.T @ dho\n",
        "            dbo += dho.sum(axis=0, keepdims=True)\n",
        "\n",
        "          \n",
        "            dxf = dhf @ Wf.T[:, :H]\n",
        "            dxi = dhi @ Wi.T[:, :H]\n",
        "            dxc = dhc @ Wc.T[:, :H]\n",
        "            dxo = dho @ Wo.T[:, :H]\n",
        "\n",
        "            delta = (dxf + dxi + dxc + dxo)  \n",
        "        grads = {\"Wf\": dWf, \"bf\": dbf,\n",
        "                 \"Wi\": dWi, \"bi\": dbi,\n",
        "                 \"Wc\": dWc, \"bc\": dbc,\n",
        "                 \"Wo\": dWo, \"bo\": dbo}\n",
        "\n",
        "        return grads\n",
        "\n",
        "\n",
        "    def forward_gru(self, X, p_fl):\n",
        "        \"\"\"\n",
        "        This is a function for implementing the forward pass of a GRU (Gated \n",
        "        Recurrent Unit) network, another type of recurrent neural network. In the \n",
        "        forward pass, the function is responsible for computing the functionOfActivations of \n",
        "        the GRU at each time step given the input data and the model's parameters \n",
        "        (i.e., the weights and biases of the GRU).\n",
        "\n",
        "        The function takes in two arguments:\n",
        "\n",
        "        X: a 3D array of shape (N, T, D), where N is the batch size, T is the \n",
        "        number of time steps, and D is the input dimension. This represents the \n",
        "        input data for the GRU, with one time step per row.\n",
        "        p_fl: a dictionary containing the weights and biases of the GRU.\n",
        "        The function first unpacks the weights and biases from the p_fl dictionary. \n",
        "        It then initializes several variables to hold the functionOfActivations of the GRU at \n",
        "        each time step and the intermediate values computed during the forward pass. \n",
        "        It then loops through the time steps, starting from the first time step and \n",
        "        going to the final time step. At each time step, it computes the functionOfActivations \n",
        "        of the update gate, reset gate, and hidden state using the sigmoid and tanh \n",
        "        functionOfActivation functions. It then updates the hidden state using the functionOfActivations \n",
        "        of the update and reset gates and the previous hidden state. Finally, it \n",
        "        updates the previous hidden state with the current hidden state.\n",
        "\n",
        "        Once the loop is complete, the function returns the final hidden state of \n",
        "        the GRU and a dictionary containing the functionOfActivations and intermediate values \n",
        "        computed during the forward pass.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        Wz = p_fl[\"Wz\"]\n",
        "        Wr = p_fl[\"Wr\"]\n",
        "        Wh = p_fl[\"Wh\"]\n",
        "\n",
        "        Uz = p_fl[\"Uz\"]\n",
        "        Ur = p_fl[\"Ur\"]\n",
        "        Uh = p_fl[\"Uh\"]\n",
        "\n",
        "        bz = p_fl[\"bz\"]\n",
        "        br = p_fl[\"br\"]\n",
        "        bh = p_fl[\"bh\"]\n",
        "\n",
        "        N, T, D = X.shape\n",
        "        H = self.size[1]\n",
        "\n",
        "        previousOf_h = np.zeros((N, H))\n",
        "\n",
        "        z = np.empty((N, T, H))\n",
        "        z_d = np.empty((N, T, H))\n",
        "        r = np.empty((N, T, H))\n",
        "        r_d = np.empty((N, T, H))\n",
        "        h_tilde = np.empty((N, T, H))\n",
        "        h_tilde_d = np.empty((N, T, H))\n",
        "        h = np.empty((N, T, H))\n",
        "\n",
        "        for t in range(T):\n",
        "            x = X[:, t, :]\n",
        "            z[:, t, :], z_d[:, t, :] = self.functionOfActivation(x @ Wz + previousOf_h @ Uz + bz, \"sigmoid\")\n",
        "            r[:, t, :], r_d[:, t, :] = self.functionOfActivation(x @ Wr + previousOf_h @ Ur + br, \"sigmoid\")\n",
        "            h_tilde[:, t, :], h_tilde_d[:, t, :] = self.functionOfActivation(x @ Wh + (r[:, t, :] * previousOf_h) @ Uh + bh, \"tanh\")\n",
        "            h[:, t, :] = (1 - z[:, t, :]) * previousOf_h + z[:, t, :] * h_tilde[:, t, :]\n",
        "\n",
        "            previousOf_h = h[:, t, :]\n",
        "\n",
        "        memory = {\"z\": (z, z_d),\n",
        "                 \"r\": (r, r_d),\n",
        "                 \"h_tilde\": (h_tilde, h_tilde_d),\n",
        "                 \"h\": h}\n",
        "\n",
        "        return previousOf_h, memory\n",
        "\n",
        "\n",
        "    def backward_gru(self, X, memory, p_fl, delta):\n",
        "        \"\"\"\n",
        "        This is a function for implementing the backward pass of a GRU network. \n",
        "        In the backward pass, the function is responsible for computing the gradients \n",
        "        of the loss function with respect to the model's parameters (i.e., the weights and biases of the GRU).\n",
        "\n",
        "        The function takes in several arguments:\n",
        "\n",
        "        X: a 3D array of shape (N, T, D), where N is the batch size, T is the number of \n",
        "        time steps, and D is the input dimension. This represents the input data for the GRU.\n",
        "        memory: a dictionary containing the functionOfActivations and intermediate values \n",
        "        computed during the forward pass of the GRU.\n",
        "\n",
        "        p_fl: a dictionary containing the weights and biases of the GRU.\n",
        "        delta: the error gradient with respect to the output of the GRU at the final time step.\n",
        "\n",
        "        The function first unpacks the weights and biases from the p_fl dictionary \n",
        "        and the functionOfActivations and intermediate values from the memory dictionary. \n",
        "        It then initializes several variables to hold the gradients of the loss \n",
        "        function with respect to the weights and biases of the GRU. It then loops \n",
        "        through the time steps in the reverse order, starting from the final time \n",
        "        step and going back to the first time step. At each time step, it computes \n",
        "        the gradients of the loss function with respect to the update gate, reset gate, \n",
        "        and hidden state using the chain rule and the derivatives of the sigmoid \n",
        "        and tanh functionOfActivation functions. It then updates the gradients of the weights \n",
        "        and biases by adding the gradients at that time step. Finally, it updates \n",
        "        the error gradient delta by adding up the gradients with respect to the \n",
        "        input to the GRU at that time step.\n",
        "\n",
        "        Once the loop is complete, the function returns a dictionary containing \n",
        "        the gradients of the loss function with respect to the weights and biases of the GRU.\n",
        "        \"\"\"\n",
        "\n",
        "       \n",
        "        Uz = p_fl[\"Uz\"]\n",
        "        Ur = p_fl[\"Ur\"]\n",
        "        Uh = p_fl[\"Uh\"]\n",
        "\n",
        "        z, z_d = memory[\"z\"]\n",
        "        r, r_d = memory[\"r\"]\n",
        "        h_tilde, h_tilde_d = memory[\"h_tilde\"]\n",
        "        h = memory[\"h\"]\n",
        "\n",
        "        H = self.size[1]\n",
        "        N, T, D = X.shape\n",
        "\n",
        "        \n",
        "        dWz = 0\n",
        "        dUz = 0\n",
        "        dbz = 0\n",
        "        dWr = 0\n",
        "        dUr = 0\n",
        "        dbr = 0\n",
        "        dWh = 0\n",
        "        dUh = 0\n",
        "        dbh = 0\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            x = X[:, t, :]\n",
        "\n",
        "            # if t = 0 we want h(t-1) = 0\n",
        "            if t > 0:\n",
        "                previousOf_h = h[:, t - 1, :]\n",
        "            else:\n",
        "                previousOf_h = np.zeros((N, H))\n",
        "\n",
        "            # similar to LSTM we find some intermediate values for each gate\n",
        "            # dE/dz is named as dz for example, this is true for all naming\n",
        "            dz = delta * (h_tilde[:, t, :] - previousOf_h) * z_d[:, t, :]\n",
        "            dh_tilde = delta * z[:, t, :] * h_tilde_d[:, t, :]\n",
        "            dr = (dh_tilde @ Uh.T) * previousOf_h * r_d[:, t, :]\n",
        "\n",
        "            # add to the sum of gradients\n",
        "            dWz += x.T @ dz\n",
        "            dUz += previousOf_h.T @ dz\n",
        "            dbz += dz.sum(axis=0, keepdims=True)\n",
        "\n",
        "            dWr += x.T @ dr\n",
        "            dUr += previousOf_h.T @ dr\n",
        "            dbr += dr.sum(axis=0, keepdims=True)\n",
        "\n",
        "            dWh += x.T @ dh_tilde\n",
        "            dUh += previousOf_h.T @ dh_tilde\n",
        "            dbh += dh_tilde.sum(axis=0, keepdims=True)\n",
        "\n",
        "            # update delta, this step uses chain rule and resultOfDerivation of multiplication, at the end it simplifies to\n",
        "            #the sum of these three terms\n",
        "            d1 = delta * (1 - z[:, t, :])\n",
        "            d2 = dz @ Uz.T\n",
        "            d3 = (dh_tilde  @ Uh.T) * (r[:, t, :] + previousOf_h * (r_d[:, t, :] @ Ur.T))\n",
        "\n",
        "            delta = d1 + d2 + d3\n",
        "\n",
        "\n",
        "        grads = {\"Wz\": dWz, \"Uz\": dUz, \"bz\": dbz,\n",
        "                 \"Wr\": dWr, \"Ur\": dUr, \"br\": dbr,\n",
        "                 \"Wh\": dWh, \"Uh\": dUh, \"bh\": dbh}\n",
        "\n",
        "        return grads\n",
        "\n",
        "\n",
        "    def cross_entropy(self, desired, output):\n",
        "      \n",
        "        return np.sum(- desired * np.log(output)) / desired.shape[0]\n",
        "\n",
        "\n",
        "    def functionOfActivation(self, X, a):\n",
        "        \"\"\"\n",
        "        This is a function for implementing various functionOfActivation functions. functionOfActivation \n",
        "        functions are mathematical functions that are used to introduce non-linearity \n",
        "        into a neural network. They take in an input value or an array of input values \n",
        "        and return an output value or an array of output values.\n",
        "\n",
        "        The function takes in two arguments:\n",
        "\n",
        "        X: an array or a scalar value representing the input to the functionOfActivation function.\n",
        "        \n",
        "        a: a string indicating the type of functionOfActivation function to use.\n",
        "        \"\"\"\n",
        "\n",
        "        if a == \"tanh\":\n",
        "            functionOfActivation = np.tanh(X)\n",
        "            resultOfDerivation = 1 - functionOfActivation ** 2\n",
        "            return functionOfActivation, resultOfDerivation\n",
        "\n",
        "        if a == \"sigmoid\":\n",
        "            functionOfActivation = 1 / (1 + np.exp(-X))\n",
        "            resultOfDerivation = functionOfActivation * (1 - functionOfActivation)\n",
        "            return functionOfActivation, resultOfDerivation\n",
        "\n",
        "        if a == \"relu\":\n",
        "            functionOfActivation = X * (X > 0)\n",
        "            resultOfDerivation = 1 * (X > 0)\n",
        "            return functionOfActivation, resultOfDerivation\n",
        "\n",
        "        if a == \"softmax\":\n",
        "            functionOfActivation = np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)\n",
        "            resultOfDerivation = None\n",
        "            return functionOfActivation, resultOfDerivation\n",
        "\n",
        "\n",
        "    def predict(self, X, Y=None, acc=True, confusion = False):\n",
        "        \"\"\"\n",
        "        This function method of a class that is used to predict labels for a given\n",
        "         set of inputs X and compare the predictions to ground truth labels Y. \n",
        "         If acc is True, the function will compute the argmax of the prediction \n",
        "         and return the accuracy, which is the percentage of times that the predicted \n",
        "         labels match the ground truth labels. If confusion is True, the function will \n",
        "         return a confusion matrix, which is a KxK matrix where K is the number of classes. \n",
        "         The matrix will have the count of how many times each class was predicted as \n",
        "         each other class.\n",
        "\n",
        "        \"\"\"\n",
        "        pred = self.forward_propagation_pass(X)[0]\n",
        "\n",
        "        if not acc:\n",
        "            return pred\n",
        "\n",
        "        pred = pred.argmax(axis=1)\n",
        "        Y = Y.argmax(axis=1)\n",
        "\n",
        "        if not confusion:\n",
        "            return (pred == Y).mean() * 100 #accuracy\n",
        "\n",
        "        K = len(np.unique(Y))  # Number of classes\n",
        "        c = np.zeros((K, K))\n",
        "\n",
        "        for i in range(len(Y)):\n",
        "            c[Y[i]][pred[i]] += 1\n",
        "\n",
        "        return c\n",
        "\n",
        "\n",
        "def q3():\n",
        "    filename = \"data3.h5\"\n",
        "    h5 = h5py.File(filename, 'r')\n",
        "    trX = h5['trX'][()].astype('float64')\n",
        "    tstX = h5['tstX'][()].astype('float64')\n",
        "    trY = h5['trY'][()].astype('float64')\n",
        "    tstY = h5['tstY'][()].astype('float64')\n",
        "    h5.close()\n",
        "\n",
        "    alpha = 0.85\n",
        "    eta = 0.01\n",
        "    epoch = 10\n",
        "    batch_size = 32\n",
        "    size = [trX.shape[2], 128, 32, 16, 6]\n",
        "\n",
        "    print(\"Recurrent Layer\\n\")\n",
        "    nn = humanActivityRecognition(size, 1)\n",
        "    listof_trainLoss, listOf_valueLoss, listOf_trainAccuracy, listOf_valueAccuracy = nn.train(trX, trY, eta, alpha, batch_size, epoch).values()\n",
        "    tst_acc = nn.predict(tstX, tstY, acc=True)\n",
        "\n",
        "    print(\"\\nTest Accuracy: \", tst_acc, \"\\n\\n\")\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 10), dpi=160, facecolor='w', edgecolor='k')\n",
        "    fig.suptitle(\"RNN\\nLearning Rate {} | action = {} | Batch Size  = {} | Hidden Layers = {}\\n\"\n",
        "                 \"Train Accuracy: {:.1f} | Validation Accuracy: {:.1f} | Test Accuracy: {:.1f}\\n \"\n",
        "                 .format(eta, alpha, batch_size, size[2:-1], listOf_trainAccuracy[-1], listOf_valueAccuracy[-1], tst_acc), fontsize=13)\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(listof_trainLoss, \"C2\", label=\"Train Cross Entropy Loss\")\n",
        "    plt.title(\"Train Cross Entropy Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(listOf_valueLoss, \"C3\", label=\"Validation Cross Entropy Loss\")\n",
        "    plt.title(\"Validation Cross Entropy Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(listOf_trainAccuracy, \"C2\", label=\"Train Accuracy\")\n",
        "    plt.title(\"Train Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(listOf_valueAccuracy, \"C3\", label=\"Validation Accuracy\")\n",
        "    plt.title(\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.savefig(\"q3a.png\", bbox_inches='tight')\n",
        "\n",
        "    train_confusion = nn.predict(trX, trY, acc=True, confusion=True)\n",
        "    test_confusion = nn.predict(tstX, tstY, acc=True, confusion=True)\n",
        "\n",
        "    plt.figure(figsize=(20, 10), dpi=160)\n",
        "\n",
        "    names = [1, 2, 3, 4, 5, 6]\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sn.heatmap(train_confusion, annot=True, annot_kws={\"size\": 8}, xticklabels=names, yticklabels=names, cmap=sn.cm.rocket_r, fmt='g')\n",
        "    plt.title(\"Train Confusion Matrix\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.xlabel(\"Prediction\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sn.heatmap(test_confusion, annot=True, annot_kws={\"size\": 8}, xticklabels=names, yticklabels=names, cmap=sn.cm.rocket_r, fmt='g')\n",
        "    plt.title(\"Test Confusion Matrix\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.xlabel(\"Prediction\")\n",
        "    plt.savefig(\"q3a_confusion.png\", bbox_inches='tight')\n",
        "\n",
        "    ##############################\n",
        "\n",
        "    alpha = 0.85\n",
        "    eta = 0.01\n",
        "    epoch = 10\n",
        "    batch_size = 32\n",
        "    size = [trX.shape[2], 128, 32, 16, 6]\n",
        "\n",
        "    print(\"\\nLSTM Layer\\n\")\n",
        "\n",
        "    nn = humanActivityRecognition(size, 2)\n",
        "    listof_trainLoss, listOf_valueLoss, listOf_trainAccuracy, listOf_valueAccuracy = nn.train(trX, trY, eta, alpha, batch_size, epoch).values()\n",
        "    tst_acc = nn.predict(tstX, tstY, acc=True)\n",
        "\n",
        "    print(\"\\nTest Accuracy: \", tst_acc, \"\\n\\n\")\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 10), dpi=160, facecolor='w', edgecolor='k')\n",
        "    fig.suptitle(\"LSTM\\nLearning Rate {} | action = {} | Batch Size  = {} | Hidden Layers = {}\\n\"\n",
        "                 \"Train Accuracy: {:.1f} | Validation Accuracy: {:.1f} | Test Accuracy: {:.1f}\\n \"\n",
        "                 .format(eta, alpha, batch_size, size[2:-1], listOf_trainAccuracy[-1], listOf_valueAccuracy[-1], tst_acc), fontsize=13)\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(listof_trainLoss, \"C2\", label=\"Train Cross Entropy Loss\")\n",
        "    plt.title(\"Train Cross Entropy Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(listOf_valueLoss, \"C3\", label=\"Validation Cross Entropy Loss\")\n",
        "    plt.title(\"Validation Cross Entropy Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(listOf_trainAccuracy, \"C2\", label=\"Train Accuracy\")\n",
        "    plt.title(\"Train Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(listOf_valueAccuracy, \"C3\", label=\"Validation Accuracy\")\n",
        "    plt.title(\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.savefig(\"q3b.png\", bbox_inches='tight')\n",
        "\n",
        "    train_confusion = nn.predict(trX, trY, acc=True, confusion=True)\n",
        "    test_confusion = nn.predict(tstX, tstY, acc=True, confusion=True)\n",
        "\n",
        "    plt.figure(figsize=(20, 10), dpi=160)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sn.heatmap(train_confusion, annot=True, annot_kws={\"size\": 8}, xticklabels=names, yticklabels=names, cmap=sn.cm.rocket_r, fmt='g')\n",
        "    plt.title(\"Train Confusion Matrix\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.xlabel(\"Prediction\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sn.heatmap(test_confusion, annot=True, annot_kws={\"size\": 8}, xticklabels=names, yticklabels=names, cmap=sn.cm.rocket_r, fmt='g')\n",
        "    plt.title(\"Test Confusion Matrix\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.xlabel(\"Prediction\")\n",
        "    plt.savefig(\"q3b_confusion.png\", bbox_inches='tight')\n",
        "\n",
        "    ##############################\n",
        "\n",
        "    alpha = 0.85\n",
        "    eta = 0.01\n",
        "    epoch = 10\n",
        "    batch_size = 32\n",
        "    size = [trX.shape[2], 128, 32, 16, 6]\n",
        "\n",
        "    print(\"\\nGRU Layer\\n\")\n",
        "\n",
        "    nn = humanActivityRecognition(size, 3)\n",
        "    listof_trainLoss, listOf_valueLoss, listOf_trainAccuracy, listOf_valueAccuracy = nn.train(trX, trY, eta, alpha, batch_size, epoch).values()\n",
        "    tst_acc = nn.predict(tstX, tstY, acc=True)\n",
        "\n",
        "    print(\"\\nTest Accuracy: \", tst_acc, \"\\n\\n\")\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 10), dpi=160, facecolor='w', edgecolor='k')\n",
        "    fig.suptitle(\"GRU\\nLearning Rate {} | action = {} | Batch Size  = {} | Hidden Layers = {}\\n\"\n",
        "                 \"Train Accuracy: {:.1f} | Validation Accuracy: {:.1f} | Test Accuracy: {:.1f}\\n \"\n",
        "                 .format(eta, alpha, batch_size, size[2:-1], listOf_trainAccuracy[-1], listOf_valueAccuracy[-1], tst_acc), fontsize=13)\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(listof_trainLoss, \"C2\", label=\"Train Cross Entropy Loss\")\n",
        "    plt.title(\"Train Cross Entropy Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(listOf_valueLoss, \"C3\", label=\"Validation Cross Entropy Loss\")\n",
        "    plt.title(\"Validation Cross Entropy Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(listOf_trainAccuracy, \"C2\", label=\"Train Accuracy\")\n",
        "    plt.title(\"Train Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(listOf_valueAccuracy, \"C3\", label=\"Validation Accuracy\")\n",
        "    plt.title(\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.savefig(\"q3c.png\", bbox_inches='tight')\n",
        "\n",
        "    train_confusion = nn.predict(trX, trY, acc=True, confusion=True)\n",
        "    test_confusion = nn.predict(tstX, tstY, acc=True, confusion=True)\n",
        "\n",
        "    plt.figure(figsize=(20, 10), dpi=160)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sn.heatmap(train_confusion, annot=True, annot_kws={\"size\": 8}, xticklabels=names, yticklabels=names, cmap=sn.cm.rocket_r, fmt='g')\n",
        "    plt.title(\"Train Confusion Matrix\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.xlabel(\"Prediction\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sn.heatmap(test_confusion, annot=True, annot_kws={\"size\": 8}, xticklabels=names, yticklabels=names, cmap=sn.cm.rocket_r, fmt='g')\n",
        "    plt.title(\"Test Confusion Matrix\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.xlabel(\"Prediction\")\n",
        "    plt.savefig(\"q3c_confusion.png\", bbox_inches='tight')\n",
        "\n",
        "\n"
      ]
    }
  ]
}